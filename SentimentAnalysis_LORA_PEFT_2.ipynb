{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install evaluate\n!pip install peft","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T18:42:17.184856Z","iopub.execute_input":"2024-12-17T18:42:17.185079Z","iopub.status.idle":"2024-12-17T18:42:29.639727Z","shell.execute_reply.started":"2024-12-17T18:42:17.185058Z","shell.execute_reply":"2024-12-17T18:42:29.638850Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: evaluate in /opt/conda/lib/python3.10/site-packages (0.4.3)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.1.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.26.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.15.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (17.0.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.5)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.6.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n^C\n\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: peft in /opt/conda/lib/python3.10/site-packages (0.14.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.2)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.4.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.46.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.4)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (1.1.1)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.5)\nRequirement already satisfied: huggingface-hub>=0.25.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.26.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.25.0->peft) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.25.0->peft) (2024.6.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.25.0->peft) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.25.0->peft) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2024.5.15)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.20.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.25.0->peft) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.25.0->peft) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.25.0->peft) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.25.0->peft) (2024.6.2)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n","output_type":"stream"}],"execution_count":38},{"cell_type":"markdown","source":"# Import libraries and load Dataset","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset, DatasetDict, Dataset\nfrom transformers import (\n\n    AutoTokenizer,\n    AutoConfig,\n    AutoModelForSequenceClassification,\n    DataCollatorWithPadding,\n    TrainingArguments,\n    Trainer\n)\nfrom peft import PeftModel, PeftConfig, get_peft_model, LoraConfig\nimport evaluate\nimport torch\nimport numpy as np\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T18:42:29.641672Z","iopub.execute_input":"2024-12-17T18:42:29.641980Z","iopub.status.idle":"2024-12-17T18:42:29.647961Z","shell.execute_reply.started":"2024-12-17T18:42:29.641951Z","shell.execute_reply":"2024-12-17T18:42:29.647088Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"model_checkpoint = \"distilbert-base-uncased\"\n\nid2label = {0:\"negative\",1:\"neutral\",2:\"positive\"}\nlabel2id = {\"negative\":0,\"neutral\":1,\"positive\":2}\n\nmodel = AutoModelForSequenceClassification.from_pretrained(model_checkpoint,num_labels=3,id2label=id2label,label2id=label2id)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T18:42:29.649010Z","iopub.execute_input":"2024-12-17T18:42:29.649356Z","iopub.status.idle":"2024-12-17T18:42:29.740926Z","shell.execute_reply.started":"2024-12-17T18:42:29.649299Z","shell.execute_reply":"2024-12-17T18:42:29.740348Z"}},"outputs":[{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"dataset = load_dataset(\"Sp1786/multiclass-sentiment-analysis-dataset\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T18:42:29.742529Z","iopub.execute_input":"2024-12-17T18:42:29.742790Z","iopub.status.idle":"2024-12-17T18:42:30.334907Z","shell.execute_reply.started":"2024-12-17T18:42:29.742766Z","shell.execute_reply":"2024-12-17T18:42:30.334029Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"#Dataset shape\ndataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T18:42:30.335802Z","iopub.execute_input":"2024-12-17T18:42:30.336124Z","iopub.status.idle":"2024-12-17T18:42:30.345248Z","shell.execute_reply.started":"2024-12-17T18:42:30.336089Z","shell.execute_reply":"2024-12-17T18:42:30.343625Z"}},"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'text', 'label', 'sentiment'],\n        num_rows: 31232\n    })\n    validation: Dataset({\n        features: ['id', 'text', 'label', 'sentiment'],\n        num_rows: 5205\n    })\n    test: Dataset({\n        features: ['id', 'text', 'label', 'sentiment'],\n        num_rows: 5206\n    })\n})"},"metadata":{}}],"execution_count":42},{"cell_type":"code","source":"dataset2 = DatasetDict({\n    \"validation\": dataset[\"validation\"],\n    \"test\": dataset[\"test\"]\n})\ndataset2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T18:42:30.347040Z","iopub.execute_input":"2024-12-17T18:42:30.347881Z","iopub.status.idle":"2024-12-17T18:42:30.356813Z","shell.execute_reply.started":"2024-12-17T18:42:30.347840Z","shell.execute_reply":"2024-12-17T18:42:30.355821Z"}},"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    validation: Dataset({\n        features: ['id', 'text', 'label', 'sentiment'],\n        num_rows: 5205\n    })\n    test: Dataset({\n        features: ['id', 'text', 'label', 'sentiment'],\n        num_rows: 5206\n    })\n})"},"metadata":{}}],"execution_count":43},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Tokenizer","metadata":{}},{"cell_type":"code","source":"#Define the tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint,add_prefix_space=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T18:42:30.357962Z","iopub.execute_input":"2024-12-17T18:42:30.358328Z","iopub.status.idle":"2024-12-17T18:42:30.462763Z","shell.execute_reply.started":"2024-12-17T18:42:30.358240Z","shell.execute_reply":"2024-12-17T18:42:30.461788Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"#model.print_trainable_parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T18:42:30.464136Z","iopub.execute_input":"2024-12-17T18:42:30.464438Z","iopub.status.idle":"2024-12-17T18:42:30.469082Z","shell.execute_reply.started":"2024-12-17T18:42:30.464411Z","shell.execute_reply":"2024-12-17T18:42:30.468233Z"}},"outputs":[],"execution_count":45},{"cell_type":"markdown","source":"# Model Performance before fine-tuning/ From raw LLM","metadata":{}},{"cell_type":"code","source":"#Untrained Model Performance\ntext_examples = [\"That food was absolutely delicious.\",\"I hate playing chess.\",\"I love watching football.\",\"The food was okay\",\"Oh of course you don't know how it works.\"]\n\nfor text in text_examples:\n    #tokenize text\n    input = tokenizer.encode(text,return_tensors=\"pt\")\n    #compute logits\n    logit = model(input).logits\n    predictions = torch.argmax(logit)\n    print(text+\"-\"+id2label[predictions.tolist()])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T18:42:30.470285Z","iopub.execute_input":"2024-12-17T18:42:30.470630Z","iopub.status.idle":"2024-12-17T18:42:30.719108Z","shell.execute_reply.started":"2024-12-17T18:42:30.470595Z","shell.execute_reply":"2024-12-17T18:42:30.718375Z"}},"outputs":[{"name":"stdout","text":"That food was absolutely delicious.-negative\nI hate playing chess.-positive\nI love watching football.-positive\nThe food was okay-neutral\nOh of course you don't know how it works.-negative\n","output_type":"stream"}],"execution_count":46},{"cell_type":"code","source":"#Tokenize entire dataset\n\ndef tokenize_function(examples):\n    texts = examples[\"text\"]\n\n    #tokenize and truncate\n    tokenizer.truncation_side = \"left\"\n    tokenized_inputs = tokenizer(\n        texts,\n        return_tensors = \"np\",\n        truncation = True,\n        max_length = 512,\n        padding=False\n    )\n    return tokenized_inputs\n#Adding PAD tokens if none exist    \nif tokenizer.pad_token is None:\n    tokenizer.add_special_tokens({\"pad_token\":\"[PAD]\"})\n    model.resize_token_embeddings(len(tokenizer))\n\n#tokenize training(validation here, due to training set having way too many samples and testing datasets)\n#tokenized_datasets = dataset2[\"validation\"].map(tokenize_function,batched=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T18:42:30.722783Z","iopub.execute_input":"2024-12-17T18:42:30.723111Z","iopub.status.idle":"2024-12-17T18:42:30.731376Z","shell.execute_reply.started":"2024-12-17T18:42:30.723075Z","shell.execute_reply":"2024-12-17T18:42:30.729894Z"}},"outputs":[],"execution_count":47},{"cell_type":"markdown","source":"# DEBUGGING ERROR","metadata":{}},{"cell_type":"code","source":"#tokenized_datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T18:42:30.732410Z","iopub.execute_input":"2024-12-17T18:42:30.732944Z","iopub.status.idle":"2024-12-17T18:42:30.741999Z","shell.execute_reply.started":"2024-12-17T18:42:30.732907Z","shell.execute_reply":"2024-12-17T18:42:30.741112Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\ndf = pd.DataFrame(dataset2[\"test\"])\ndf[\"text\"].isna().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T18:42:30.743930Z","iopub.execute_input":"2024-12-17T18:42:30.744238Z","iopub.status.idle":"2024-12-17T18:42:31.012844Z","shell.execute_reply.started":"2024-12-17T18:42:30.744204Z","shell.execute_reply":"2024-12-17T18:42:31.011949Z"}},"outputs":[{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"1"},"metadata":{}}],"execution_count":49},{"cell_type":"code","source":"dataset2[\"test\"] = dataset2[\"test\"].filter(lambda example: pd.notna(example[\"text\"]))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T18:42:31.013983Z","iopub.execute_input":"2024-12-17T18:42:31.014620Z","iopub.status.idle":"2024-12-17T18:42:31.023612Z","shell.execute_reply.started":"2024-12-17T18:42:31.014577Z","shell.execute_reply":"2024-12-17T18:42:31.022715Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"dataset2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T18:42:31.024763Z","iopub.execute_input":"2024-12-17T18:42:31.025114Z","iopub.status.idle":"2024-12-17T18:42:31.033645Z","shell.execute_reply.started":"2024-12-17T18:42:31.025077Z","shell.execute_reply":"2024-12-17T18:42:31.032781Z"}},"outputs":[{"execution_count":51,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    validation: Dataset({\n        features: ['id', 'text', 'label', 'sentiment'],\n        num_rows: 5205\n    })\n    test: Dataset({\n        features: ['id', 'text', 'label', 'sentiment'],\n        num_rows: 5205\n    })\n})"},"metadata":{}}],"execution_count":51},{"cell_type":"code","source":"#Tokenize entire dataset\n\ndef tokenize_function(examples):\n    texts = examples[\"text\"]\n\n    #tokenize and truncate\n    tokenizer.truncation_side = \"left\"\n    tokenized_inputs = tokenizer(\n        texts,\n        return_tensors = \"np\",\n        truncation = True,\n        max_length = 512,\n        padding=False\n    )\n    return tokenized_inputs\n#Adding PAD tokens if none exist    \nif tokenizer.pad_token is None:\n    tokenizer.add_special_tokens({\"pad_token\":\"[PAD]\"})\n    model.resize_token_embeddings(len(tokenizer))\n\n#tokenize training(validation here, due to training set having way too many samples and testing datasets)\ntokenized_datasets = dataset2.map(tokenize_function,batched=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T18:42:31.034767Z","iopub.execute_input":"2024-12-17T18:42:31.035176Z","iopub.status.idle":"2024-12-17T18:42:31.468088Z","shell.execute_reply.started":"2024-12-17T18:42:31.035136Z","shell.execute_reply":"2024-12-17T18:42:31.467035Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5205 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26ff6492243d47d69bd2b2647598246e"}},"metadata":{}}],"execution_count":52},{"cell_type":"code","source":"tokenized_datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T18:42:31.469220Z","iopub.execute_input":"2024-12-17T18:42:31.469514Z","iopub.status.idle":"2024-12-17T18:42:31.475674Z","shell.execute_reply.started":"2024-12-17T18:42:31.469487Z","shell.execute_reply":"2024-12-17T18:42:31.474806Z"}},"outputs":[{"execution_count":53,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    validation: Dataset({\n        features: ['id', 'text', 'label', 'sentiment', 'input_ids', 'attention_mask'],\n        num_rows: 5205\n    })\n    test: Dataset({\n        features: ['id', 'text', 'label', 'sentiment', 'input_ids', 'attention_mask'],\n        num_rows: 5205\n    })\n})"},"metadata":{}}],"execution_count":53},{"cell_type":"markdown","source":"# ERROR RESOLVED, one set of data had 1 null value","metadata":{}},{"cell_type":"code","source":"#Data collator for efficient and dynamic padding\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T18:42:31.476726Z","iopub.execute_input":"2024-12-17T18:42:31.476982Z","iopub.status.idle":"2024-12-17T18:42:31.487809Z","shell.execute_reply.started":"2024-12-17T18:42:31.476958Z","shell.execute_reply":"2024-12-17T18:42:31.487105Z"}},"outputs":[],"execution_count":54},{"cell_type":"markdown","source":"# MODEL","metadata":{}},{"cell_type":"code","source":"#Define evaluation metrics\n\naccuracy = evaluate.load(\"accuracy\")\n\n#An evaluation function to pass into trainer\ndef compute_metrics(p):\n    predictions,labels = p\n    predictions = np.argmax(predictions,axis=1)\n\n    return {\"accuracy\":accuracy.compute(predictions=predictions,references=labels)}\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T18:42:31.488879Z","iopub.execute_input":"2024-12-17T18:42:31.489475Z","iopub.status.idle":"2024-12-17T18:42:31.776201Z","shell.execute_reply.started":"2024-12-17T18:42:31.489437Z","shell.execute_reply":"2024-12-17T18:42:31.775594Z"}},"outputs":[],"execution_count":55},{"cell_type":"code","source":"#PEFT using LORA for finetuning\n\npeft_config = LoraConfig(task_type=\"SEQ_CLS\",\n                        r=4,#intrinsic rank or rank of decomposition\n                        lora_alpha=32,\n                        lora_dropout = 0.01,\n                        target_modules=[\"q_lin\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T18:42:31.776950Z","iopub.execute_input":"2024-12-17T18:42:31.777184Z","iopub.status.idle":"2024-12-17T18:42:31.782062Z","shell.execute_reply.started":"2024-12-17T18:42:31.777161Z","shell.execute_reply":"2024-12-17T18:42:31.781326Z"}},"outputs":[],"execution_count":56},{"cell_type":"code","source":"model = get_peft_model(model,peft_config)\nmodel.print_trainable_parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T18:42:31.783201Z","iopub.execute_input":"2024-12-17T18:42:31.783556Z","iopub.status.idle":"2024-12-17T18:42:31.806194Z","shell.execute_reply.started":"2024-12-17T18:42:31.783521Z","shell.execute_reply":"2024-12-17T18:42:31.805425Z"}},"outputs":[{"name":"stdout","text":"trainable params: 629,763 || all params: 67,585,542 || trainable%: 0.9318\n","output_type":"stream"}],"execution_count":57},{"cell_type":"code","source":"# hyperparameters\nlr = 1e-3  # size of optimization step\nbatch_size = 4  # number of examples processed per optimization step\nnum_epochs = 10  # number of times model runs through training data\n\n# define training arguments\ntraining_args = TrainingArguments(\n    output_dir = \"/kaggle/working/\" + model_checkpoint + \"-lora-text-classification\",\n    learning_rate=lr,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    num_train_epochs=num_epochs,\n    weight_decay=0.01,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T18:42:31.807235Z","iopub.execute_input":"2024-12-17T18:42:31.807591Z","iopub.status.idle":"2024-12-17T18:42:31.849146Z","shell.execute_reply.started":"2024-12-17T18:42:31.807554Z","shell.execute_reply":"2024-12-17T18:42:31.848363Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"}],"execution_count":58},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,  \n    args=training_args, \n    train_dataset=tokenized_datasets[\"validation\"],#using validation for training as actual training set is pretty heavy, we dont need those many samples  \n    eval_dataset=tokenized_datasets[\"test\"],\n    tokenizer=tokenizer, \n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T18:42:31.850139Z","iopub.execute_input":"2024-12-17T18:42:31.850420Z","iopub.status.idle":"2024-12-17T18:42:31.935421Z","shell.execute_reply.started":"2024-12-17T18:42:31.850396Z","shell.execute_reply":"2024-12-17T18:42:31.934597Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_23/1145778912.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"}],"execution_count":59},{"cell_type":"code","source":"import os\nos.environ[\"WANDB_DISABLED\"] = \"true\"#I don't have a Weights&Biases api key :/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T18:42:31.936413Z","iopub.execute_input":"2024-12-17T18:42:31.936651Z","iopub.status.idle":"2024-12-17T18:42:31.941065Z","shell.execute_reply.started":"2024-12-17T18:42:31.936627Z","shell.execute_reply":"2024-12-17T18:42:31.940146Z"}},"outputs":[],"execution_count":60},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T18:42:31.942141Z","iopub.execute_input":"2024-12-17T18:42:31.942447Z","iopub.status.idle":"2024-12-17T18:55:57.519960Z","shell.execute_reply.started":"2024-12-17T18:42:31.942406Z","shell.execute_reply":"2024-12-17T18:55:57.519244Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='6510' max='6510' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [6510/6510 13:24, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.803900</td>\n      <td>0.727763</td>\n      <td>{'accuracy': 0.6806916426512968}</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.657800</td>\n      <td>0.700642</td>\n      <td>{'accuracy': 0.7014409221902017}</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.638500</td>\n      <td>0.783041</td>\n      <td>{'accuracy': 0.7064361191162344}</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.535400</td>\n      <td>0.823210</td>\n      <td>{'accuracy': 0.7125840537944285}</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.501100</td>\n      <td>0.851971</td>\n      <td>{'accuracy': 0.7056676272814602}</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.447400</td>\n      <td>0.982143</td>\n      <td>{'accuracy': 0.7068203650336216}</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.377200</td>\n      <td>1.006208</td>\n      <td>{'accuracy': 0.7093179634966379}</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.333800</td>\n      <td>1.080426</td>\n      <td>{'accuracy': 0.7081652257444765}</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.302900</td>\n      <td>1.128694</td>\n      <td>{'accuracy': 0.7116234390009606}</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.263200</td>\n      <td>1.165566</td>\n      <td>{'accuracy': 0.7121998078770413}</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Trainer is attempting to log a value of \"{'accuracy': 0.6806916426512968}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nTrainer is attempting to log a value of \"{'accuracy': 0.7014409221902017}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nTrainer is attempting to log a value of \"{'accuracy': 0.7064361191162344}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nTrainer is attempting to log a value of \"{'accuracy': 0.7125840537944285}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nTrainer is attempting to log a value of \"{'accuracy': 0.7056676272814602}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nTrainer is attempting to log a value of \"{'accuracy': 0.7068203650336216}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nTrainer is attempting to log a value of \"{'accuracy': 0.7093179634966379}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nTrainer is attempting to log a value of \"{'accuracy': 0.7081652257444765}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nTrainer is attempting to log a value of \"{'accuracy': 0.7116234390009606}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nTrainer is attempting to log a value of \"{'accuracy': 0.7121998078770413}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","output_type":"stream"},{"execution_count":61,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=6510, training_loss=0.475362392028539, metrics={'train_runtime': 805.101, 'train_samples_per_second': 64.65, 'train_steps_per_second': 8.086, 'total_flos': 888680796217200.0, 'train_loss': 0.475362392028539, 'epoch': 10.0})"},"metadata":{}}],"execution_count":61},{"cell_type":"code","source":"\n\"\"\"\nTrainOutput(global_step=6510, training_loss=0.475362392028539, metrics={'train_runtime': 805.101, 'train_samples_per_second': 64.65, 'train_steps_per_second': 8.086, 'total_flos': 888680796217200.0, 'train_loss': 0.475362392028539, 'epoch': 10.0})\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T19:04:41.721668Z","iopub.execute_input":"2024-12-17T19:04:41.722401Z","iopub.status.idle":"2024-12-17T19:04:41.728565Z","shell.execute_reply.started":"2024-12-17T19:04:41.722368Z","shell.execute_reply":"2024-12-17T19:04:41.727630Z"}},"outputs":[{"execution_count":72,"output_type":"execute_result","data":{"text/plain":"\"\\nTrainOutput(global_step=6510, training_loss=0.475362392028539, metrics={'train_runtime': 805.101, 'train_samples_per_second': 64.65, 'train_steps_per_second': 8.086, 'total_flos': 888680796217200.0, 'train_loss': 0.475362392028539, 'epoch': 10.0})\\n\""},"metadata":{}}],"execution_count":72},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Testing out our Model","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\ntext_examples = [\n    \"That food was absolutely delicious.\",\n    \"I hate playing chess.\",\n    \"I love watching football.\",\n    \"The food was okay\",\n    \"Oh of course you don't know how it works.\"\n]\n\nfor text in text_examples:\n    # Tokenize text\n    input = tokenizer.encode(text, return_tensors=\"pt\").to(device)\n    \n    # Compute logits\n    logit = model(input).logits\n    \n    # Compute predictions\n    predictions = torch.argmax(logit, dim=-1)\n    \n    # Print results\n    print(text + \" - \" + id2label[predictions.item()])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T19:00:26.396698Z","iopub.execute_input":"2024-12-17T19:00:26.397521Z","iopub.status.idle":"2024-12-17T19:00:26.584803Z","shell.execute_reply.started":"2024-12-17T19:00:26.397473Z","shell.execute_reply":"2024-12-17T19:00:26.583876Z"}},"outputs":[{"name":"stdout","text":"That food was absolutely delicious. - positive\nI hate playing chess. - negative\nI love watching football. - positive\nThe food was okay - neutral\nOh of course you don't know how it works. - negative\n","output_type":"stream"}],"execution_count":65},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
